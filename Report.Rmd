---
title: "Markowitz Mean-Variance Optimization (MVO) Portfolio"
author: "Rudraksha Dev Sharma"
output:
  github_document: default
  html_document:
    toc: true
    number_sections: true
    df_print: paged
---

# Fetching Necessary Libraries
In this step, we simply fetch the necessary libraries

```{r Fetch Libraries}
library(tidyquant)
library(ggplot2)
library(corrplot)
library(dplyr)
library(quadprog)
```

# Symbols
The symbols that we have decided to use here are SPY (for the S&P500), FXI (for iShares China Large-Cap ETF), GLD (for Gold), and EPI (for Nifty50 ETF) and we'll be training it based on 17 years of data

```{r Symbols}
ticker_FXI <- "FXI" #iShares China Large-Cap
ticker_SnP <- "SPY"
ticker_Nifty50 <- "EPI"
ticker_Gold <- "GLD"

start_date = as.Date("2008-09-17")
end_date = as.Date("2025-01-01")

TR_FXI <- tq_get(ticker_FXI, from=start_date, to=end_date)
TR_SnP <- tq_get(ticker_SnP, from=start_date, to=end_date)
TR_Nifty <- tq_get(ticker_Nifty50, from = start_date, to=end_date)
TR_Gold <- tq_get(ticker_Gold, from = start_date, to = end_date)
```

# Sanity check
Purpose of this cell is to get all the returns of the 4 assets so that we can later put them all into the dataframe and also just to see whether the returns are normally distributed or not (of course they are, but I just like looking at the Law of Large Numbers come into play).

```{r Verifying returns being normal}
rets_SnP <- TR_SnP %>%
  arrange(date) %>%
  mutate(ret = adjusted/lag(adjusted) - 1) %>%
  filter(!is.na(ret))

rets_FXI <- TR_FXI %>%
  arrange(date) %>%
  mutate(ret = adjusted/lag(adjusted) - 1) %>%
  filter(!is.na(ret))

rets_Gold <- TR_Gold %>%
  arrange(date) %>%
  mutate(ret = adjusted/lag(adjusted) - 1) %>%
  filter(!is.na(ret))

rets_Nifty <- TR_Nifty %>%
  arrange(date) %>%
  mutate(ret = adjusted/lag(adjusted) - 1) %>%
  filter(!is.na(ret))

ggplot(rets_SnP, aes(x=ret)) +
  geom_histogram(bins = 10, binwidth = 0.001) +
  labs(
    title = "Returns of SnP500",
    x = "P[t]/P[t-1] - 1",
    y = "Frequency"
  ) +
  theme_minimal()

ggplot(rets_Nifty, aes(x=ret)) +
  geom_histogram(bins = 10, binwidth = 0.001) +
  labs(
    title = "Returns of Nifty50",
    x = "P[t]/P[t-1] - 1",
    y = "Frequency"
  ) +
  theme_minimal()

ggplot(rets_Gold, aes(x=ret)) +
  geom_histogram(bins = 10, binwidth = 0.001) +
  labs(
    title = "Returns of Gold",
    x = "P[t]/P[t-1] - 1",
    y = "Frequency"
  ) +
  theme_minimal()

ggplot(rets_FXI, aes(x=ret)) +
  geom_histogram(bins = 10, binwidth = 0.001) +
  labs(
    title = "Returns of FXI",
    x = "(P[t]/P[t-1]) - 1",
    y = "Frequency"
  ) +
  theme_minimal()

```
# Function for Markowitz Model
This is the main function that runs the Markowitz model for portfolio optimization. We take a small epsilon nudge to avoid the corner portfolios and use quadratic programming to minimize t(w) * Cov * w with the constraint that each individual weight of an asset lies between 0 and 1.

```{r Creating the Function}
markowitz <- function(Cov, Mu, Npoints)
{
  epsilon <- 0.0001
  Nu <- length(Mu)

  mu_min <- min(Mu) + epsilon*(mean(Mu) - min(Mu))
  mu_max <- max(Mu) - epsilon*(max(Mu) - mean(Mu))
  
  iota <- matrix(1, Nu, 1)
  zero <- matrix(0, Nu, 1)
  Amat <- cbind(iota, Mu, diag(1, Nu), diag(-1, Nu))
  meq <- 2
  
  mu_dist <- seq(from = mu_min, to = mu_max, length = Npoints)
  
  sigma_vec <- matrix(0, Npoints, 1)
  weights_mat <- matrix(0, Nu, Npoints)
  
  for(k in 1:Npoints)
  {
    bvec <- c(1, mu_dist[k], zero, -iota)
    opt <- solve.QP(Cov, zero, Amat, bvec, meq = meq)
    sigma_vec[k] <- sqrt(2* opt$value)
    weights_mat[,k] <- opt$solution
  }
  
  mu_p <- t(weights_mat) %*% Mu
  sigma_p <- sqrt(diag(t(weights_mat) %*% Cov %*% weights_mat))
  
  markowitz <- data.frame(mu_p, sigma_p, weights = t(weights_mat))
}

```

# Daily returns datframe
The chunk performs inner join on all the returns vectors of the assets and aligns them by date. This avoids any error that may arise due to missing dates if we were to simply use the cbind() function.

```{r Running the Function}
rets_all <- rets_FXI %>%
  select(date, FXI = ret) %>%
  inner_join(rets_SnP   %>% select(date, SnP   = ret), by = "date") %>%
  inner_join(rets_Nifty %>% select(date, Nifty = ret), by = "date") %>%
  inner_join(rets_Gold  %>% select(date, Gold  = ret), by = "date")

R_daily <- rets_all %>% select(-date)  # numeric matrix-like data frame (T x 4)
print(head(R_daily))
```

# Daily returns and covariance
Here, we calculate the mean daily returns of each asset and also the covariance matrix of the daily returns.

```{r Daily vectors}
Mu_daily  <- colMeans(R_daily)
Cov_daily <- cov(R_daily)

print(Mu_daily)
print(Cov_daily)
print(cor(R_daily))

corrplot(cor(R_daily), type = "lower")
```
# Annualizing Mu and Cov
Earlier we had gotten the daily Mu and Covariance matrix, now we annualize them by multiplying each by 252 (number of days in the year).
If we wished to annualize the volatility alone, we would have multiplied by sqrt(252), but for covariance and variance we would have to multiply by 252 (because it is essentially multiplying by (sqrt(252))^2).

```{r Annualizing Mu and Cov}
Mu      <- as.numeric(Mu_daily) * 252
Cov_ann <- Cov_daily * 252

print(Mu)
print(Cov_ann)

asset_names <- colnames(R_daily)
names(Mu) <- asset_names
```

# Running Markowitz Function
This chunk runs the Markowitz function Npoints number of times to create an efficient frontier. "frontier" stores the matrix of weights so that it can later be used to plot out the efficient frontier.

```{r Running Markowitz function}
Npoints <- 100
frontier <- markowitz(Cov_ann, Mu, Npoints = Npoints)

w_cols <- 3:ncol(frontier)
colnames(frontier)[w_cols] <- paste0("w_", asset_names)
```

# Monte Carlo Simulation
Now, we do Monte Carlo Simulation of various weights. We do the simulation Nsims number of times, each time taking a new random arrangement of weights. We also set a seed for the results so that they can be reproducible.

```{r Monte Carlo Simulation}
set.seed(42)
Nsims <- 20000
n <- length(Mu)

W <- matrix(rgamma(Nsims * n, shape = 1, rate = 1), nrow = Nsims, ncol = n)
W <- W / rowSums(W)
print(head(W))

mu_sim <- as.numeric(W %*% Mu)
sig_sim <- sqrt(rowSums((W %*% Cov_ann) * W))   # efficient diag(W %*% Cov %*% t(W))
```

# Sharpe Ratio Calculation
Using the portfolios we stored in the weight matrix W and calculating the sharpe ratio of each portfolio by using mu_sim and sigma_sim that we obtained in the previous chunk. We can then easily find the portfolio with the highest sharpe.

```{r Portfolio with best sharpe}
# (Optional) Sharpe ratio ranking (We can set rf if we want)
rf <- 0
sharpe_sim <- (mu_sim - rf) / sig_sim

sim_df <- data.frame(
  mu_p = mu_sim,
  sigma_p = sig_sim,
  sharpe = sharpe_sim,
  W
)
colnames(sim_df)[4:(3+n)] <- paste0("w_", asset_names)

best_idx <- which.max(sim_df$sharpe)
best_mc <- sim_df[best_idx, ]

print("Best (approx) Sharpe portfolio from Monte Carlo:")
print(best_mc)
```

# Plotting of graphs
The purpose of this chunk is purely for visualisation purposes. The red line shows the efficient frontier we calculated using our Markowitz function, the black dots show the various portfolios we obtained by doing Monte Carlo simulation, and the big green dot shows the simulated portfolio with the highest sharpe.

```{r Plotting graphs of Monte Carlo Simulation}
frontier2 <- frontier %>%
  dplyr::mutate(mu_p = as.numeric(mu_p),
                sigma_p = as.numeric(sigma_p)) %>%
  dplyr::arrange(mu_p)

i_gmv <- which.min(frontier2$sigma_p)
frontier_eff <- frontier2[i_gmv:nrow(frontier2), ] %>%
  dplyr::arrange(sigma_p)

ggplot(sim_df, aes(x = sigma_p, y = mu_p)) +
  geom_point(alpha = 0.2) +
  geom_point(data = best_mc, aes(x = sigma_p, y = mu_p), size = 3, color = "green") +
  geom_path(data = frontier2, aes(x = sigma_p, y = mu_p), linewidth = 1, color = "red") +
  labs(
    title = "Monte Carlo Portfolios + Efficient Frontier",
    x = "Annualized Volatility (sigma)",
    y = "Annualized Expected Return (mu)"
  ) +
  theme_minimal()
```

# Comparing the Results
Now, we would like to compare how the two models of ours (Monte Carlo and Markowitz) perform with respect to eachother.

```{r Calculating Deviation}
sharpe_markowitz <- (frontier2$mu_p - rf)/frontier2$sigma_p

best_markowitz_sharpe <- max(sharpe_markowitz)

print(paste("Best sharpe by Markowitz model is: ", best_markowitz_sharpe))
print(paste("Best sharpe by Monte Carlo model is: ", best_mc$sharpe))

deviation_pct <- abs(best_markowitz_sharpe - best_mc$sharpe)/best_markowitz_sharpe * 100

print(paste("Our Monte Carlo simulation deviates from the Monte Carlo model by ", deviation_pct ,"%"))
```
